# Tech Challenge - Fase 2: Pipeline Batch Bovespa

![Python](https://img.shields.io/badge/Python-3.9+-blue?style=for-the-badge&logo=python)
![Terraform](https://img.shields.io/badge/Terraform-1.0+-purple?style=for-the-badge&logo=terraform)
![AWS](https://img.shields.io/badge/AWS-Cloud-orange?style=for-the-badge&logo=amazon-aws)
![Apache Spark](https://img.shields.io/badge/Apache_Spark-Glue-red?style=for-the-badge&logo=apachespark)
![Status](https://img.shields.io/badge/Status-ConcluÃ­do-brightgreen?style=for-the-badge)

> Pipeline de dados robusto e totalmente automatizado para extrair, processar e analisar dados do pregÃ£o da B3 utilizando arquitetura de Data Lake na AWS.

Este projeto consiste na implementaÃ§Ã£o de uma arquitetura de Big Data Serverless na AWS para o processamento batch de dados financeiros. O objetivo principal Ã© garantir que os dados brutos capturados via web scraping sejam transformados em informaÃ§Ãµes valiosas, limpas e particionadas de forma eficiente para anÃ¡lise via SQL no Amazon Athena.

---

## ðŸ›ï¸ Arquitetura e Pipeline

O projeto segue a arquitetura **Medallion (Raw e Refined)**, garantindo a linhagem e a qualidade do dado em cada etapa:



### Fluxo de Dados Passo a Passo:
1.  **IngestÃ£o (Camada Raw):** Um script Python (`scraper.py`) realiza o scrap da API oficial da B3, trata os tipos numÃ©ricos e salva o arquivo no S3 em formato Parquet com partiÃ§Ã£o diÃ¡ria (`year/month/day`).
2.  **Gatilho (Trigger):** O upload do arquivo para a pasta `raw/` dispara uma **AWS Lambda** via S3 Event Notifications.
3.  **OrquestraÃ§Ã£o:** A funÃ§Ã£o Lambda, agindo de forma serverless, inicia o Job de ETL no **AWS Glue**.
4.  **Processamento ETL:** O Job do Glue executa um script PySpark que realiza transformaÃ§Ãµes de negÃ³cio, como agrupamentos, renomeaÃ§Ã£o de colunas e cÃ¡lculos de datas.
5.  **Armazenamento (Camada Refined):** Os dados processados sÃ£o salvos em formato Parquet na pasta `refined`, particionados por data e pelo ticker da aÃ§Ã£o para otimizaÃ§Ã£o de custos e performance.
6.  **CatÃ¡logo e Consulta:** O Glue Job cataloga automaticamente os metadados no **Glue Catalog**, disponibilizando os dados para consultas SQL imediatas no **Amazon Athena**.

---

## ðŸ› ï¸ Tecnologias e Ferramentas AWS

O projeto foi construÃ­do utilizando ferramentas de ponta para garantir escalabilidade e baixo custo operacional:

| Ferramenta | DescriÃ§Ã£o |
| :--- | :--- |
| **Python / PySpark** | Linguagem utilizada para extraÃ§Ã£o (Scraping), orquestraÃ§Ã£o (Lambda) e processamento (Glue). |
| **Terraform** | Utilizado para o provisionamento de toda a infraestrutura como cÃ³digo (IaC), garantindo reprodutibilidade. |
| **AWS S3** | Data Lake responsÃ¡vel pelo armazenamento das camadas Raw e Refined. |
| **AWS Lambda** | ServiÃ§o serverless para orquestraÃ§Ã£o do pipeline baseada em eventos. |
| **AWS Glue** | Motor de processamento Spark para transformaÃ§Ãµes batch complexas. |
| **Amazon Athena** | Interface de consulta SQL interativa sobre os dados do S3. |

---

## âš™ï¸ LÃ³gica de NegÃ³cio e TransformaÃ§Ãµes (Requisito 5)

Dentro do **AWS Glue Job**, as seguintes transformaÃ§Ãµes sÃ£o aplicadas para atender aos requisitos do projeto:

* **A: Agrupamento e Soma:** UtilizaÃ§Ã£o de `Window Functions` para calcular a soma da participaÃ§Ã£o por ticker.
* **B: PadronizaÃ§Ã£o:** RenomeaÃ§Ã£o de colunas tÃ©cnicas para nomes amigÃ¡veis (ex: `cod` para `ticker`).
* **C: InteligÃªncia de Datas:** CÃ¡lculo de diferenÃ§a de dias entre a data do pregÃ£o e a data atual para anÃ¡lise de defasagem.

---

## ðŸ“¦ Como Instalar e Rodar

### PrÃ©-requisitos
* **AWS CLI** configurada com credenciais administrativas.
* **Terraform** e **Python 3.9+** instalados.

## ðŸ›ï¸ Provisionamento
```bash
cd src/terraform
terraform init
terraform apply -auto-approve

```

## ðŸ›ï¸ Arquitetura

Aqui estÃ¡ o diagrama arquitetural do projeto, mostrando o fluxo de dados:

```mermaid
graph LR;
    subgraph "Fonte Externa"
        A["Site B3 (Dados do PregÃ£o)"]
    end

    subgraph "IngestÃ£o e OrquestraÃ§Ã£o"
        B["Script Scraper (Python)"]
        C[("Amazon S3 (Raw Bucket)")]
        D["AWS Lambda (Trigger)"]
    end

    subgraph "Processamento (ETL)"
        E["AWS Glue Job (Visual Spark ETL)"]
        F[("Amazon S3 (Refined Bucket)")]
    end

    subgraph "CatÃ¡logo e Consumo"
        G["AWS Glue Data Catalog"]
        H["Amazon Athena (SQL Query)"]
        I["Cliente Final (Analistas / BI)"]
    end

    %% Fluxos de Dados
    A -- "1. Scraping de Dados" --> B
    B -- "2. IngestÃ£o Parquet (PartiÃ§Ã£o DiÃ¡ria)" --> C
    C -- "3. Evento de Upload" --> D
    D -- "4. Inicia Job do Glue" --> E
    E -- "5. TransformaÃ§Ãµes (Soma, Rename, Datas)" --> F
    E -- "6. Registro de Metadados" --> G
    F -- "7. Leitura de Dados Refinados" --> H
    G -- "8. Esquema da Tabela" --> H
    H -- "9. Dashboards e AnÃ¡lises" --> I