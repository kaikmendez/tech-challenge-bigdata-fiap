# Tech Challenge - Fase 2: Pipeline Batch Bovespa

![Python](https://img.shields.io/badge/Python-3.9+-blue?style=for-the-badge&logo=python)
![Terraform](https://img.shields.io/badge/Terraform-1.0+-purple?style=for-the-badge&logo=terraform)
![AWS](https://img.shields.io/badge/AWS-Cloud-orange?style=for-the-badge&logo=amazon-aws)
![Apache Spark](https://img.shields.io/badge/Apache_Spark-Glue-red?style=for-the-badge&logo=apachespark)
![Status](https://img.shields.io/badge/Status-Conclu√≠do-brightgreen?style=for-the-badge)

> Pipeline de dados robusto e totalmente automatizado para extrair, processar e analisar dados do preg√£o da B3 utilizando arquitetura de Data Lake na AWS.

Este projeto consiste na implementa√ß√£o de uma arquitetura de Big Data Serverless na AWS para o processamento batch de dados financeiros. O objetivo principal √© garantir que os dados brutos capturados via web scraping sejam transformados em informa√ß√µes valiosas, limpas e particionadas de forma eficiente para an√°lise via SQL no Amazon Athena.

---

## üèõÔ∏è Arquitetura e Pipeline

O projeto segue a arquitetura **Medallion (Raw e Refined)**, garantindo a linhagem e a qualidade do dado em cada etapa:



### Fluxo de Dados Passo a Passo:
1.  **Ingest√£o (Camada Raw):** Um script Python (`scraper.py`) realiza o scrap da API oficial da B3, trata os tipos num√©ricos e salva o arquivo no S3 em formato Parquet com parti√ß√£o di√°ria (`year/month/day`).
2.  **Gatilho (Trigger):** O upload do arquivo para a pasta `raw/` dispara uma **AWS Lambda** via S3 Event Notifications.
3.  **Orquestra√ß√£o:** A fun√ß√£o Lambda, agindo de forma serverless, inicia o Job de ETL no **AWS Glue**.
4.  **Processamento ETL:** O Job do Glue executa um script PySpark que realiza transforma√ß√µes de neg√≥cio, como agrupamentos, renomea√ß√£o de colunas e c√°lculos de datas.
5.  **Armazenamento (Camada Refined):** Os dados processados s√£o salvos em formato Parquet na pasta `refined`, particionados por data e pelo ticker da a√ß√£o para otimiza√ß√£o de custos e performance.
6.  **Cat√°logo e Consulta:** O Glue Job cataloga automaticamente os metadados no **Glue Catalog**, disponibilizando os dados para consultas SQL imediatas no **Amazon Athena**.

---

## üõ†Ô∏è Tecnologias e Ferramentas AWS

O projeto foi constru√≠do utilizando ferramentas de ponta para garantir escalabilidade e baixo custo operacional:

| Ferramenta | Descri√ß√£o |
| :--- | :--- |
| **Python / PySpark** | Linguagem utilizada para extra√ß√£o (Scraping), orquestra√ß√£o (Lambda) e processamento (Glue). |
| **Terraform** | Utilizado para o provisionamento de toda a infraestrutura como c√≥digo (IaC), garantindo reprodutibilidade. |
| **AWS S3** | Data Lake respons√°vel pelo armazenamento das camadas Raw e Refined. |
| **AWS Lambda** | Servi√ßo serverless para orquestra√ß√£o do pipeline baseada em eventos. |
| **AWS Glue** | Motor de processamento Spark para transforma√ß√µes batch complexas. |
| **Amazon Athena** | Interface de consulta SQL interativa sobre os dados do S3. |

---

## ‚öôÔ∏è L√≥gica de Neg√≥cio e Transforma√ß√µes (Requisito 5)

Dentro do **AWS Glue Job**, as seguintes transforma√ß√µes s√£o aplicadas para atender aos requisitos do projeto:

* **A: Agrupamento e Soma:** Utiliza√ß√£o de `Window Functions` para calcular a soma da participa√ß√£o por ticker.
* **B: Padroniza√ß√£o:** Renomea√ß√£o de colunas t√©cnicas para nomes amig√°veis (ex: `cod` para `ticker`).
* **C: Intelig√™ncia de Datas:** C√°lculo de diferen√ßa de dias entre a data do preg√£o e a data atual para an√°lise de defasagem.

---

## üì¶ Como Instalar e Rodar

### Pr√©-requisitos
* **AWS CLI** configurada com credenciais administrativas.
* **Terraform** e **Python 3.9+** instalados.

### 1. Provisionamento
```bash
cd src/terraform
terraform init
terraform apply -auto-approve